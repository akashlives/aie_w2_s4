{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "✋BREAKOUT ROOM #2:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: LCEL RAG Chain\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "- Task 6: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhSjB1O6-Y0J",
        "outputId": "1f43fdba-f1b4-4cf4-b8b0-3469edc29c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.9/258.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-aiplatform 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere lxml -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "927fd78b-8510-4bea-9e68-a3c74d3eb5a1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#import getpass\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923xinz42sWV"
      },
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m9U0SbQN2sWc"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #2: Create a Simple RAG Application Using Qdrant, Hugging Face, and LCEL\n",
        "\n",
        "Now that we have a grasp on how LCEL works, and how we can use LangChain and Hugging Face to interact with our data - let's step it up a notch and incorporate Qdrant!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangChain Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using QDrant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs - which will serve as our data for today!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `SitemapLoader` to load our PDF directly from the web!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHA9L3Jxo3r",
        "outputId": "c3535941-ce13-4ddd-ef08-b6b4ab1e507b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "Fetching pages: 100%|##########| 220/220 [00:14<00:00, 15.02it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://blog.langchain.dev/customers-podium/', 'loc': 'https://blog.langchain.dev/customers-podium/', 'lastmod': '2024-08-15T14:00:05.000Z'}, page_content=\"\\n\\n\\nHow Podium optimized agent behavior and reduced engineering intervention by 90% with LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Podium optimized agent behavior and reduced engineering intervention by 90% with LangSmith\\nSee how Podium tests across the lifecycle development of their AI employee agent, using LangSmith for dataset curation and finetuning. They improved agent F1 response quality to 98% and reduced the need for engineering intervention by 90%.\\n\\n5 min read\\nAug 15, 2024\\n\\n\\n\\n\\n\\nAbout PodiumPodium is a communication platform that helps small businesses connect quickly with customers via phone, text, email, and social media. Small businesses often have high-touch interactions with customers — think automotive dealers, jewelers, bike shops — yet are understaffed. Podium's mission is to help these businesses respond to customer inquiries promptly so that they can convert leads into sales.\\xa0Podium data shows that responding to customer inquiries within 5 minutes results in a 46% higher lead conversion rate than responding in an hour. To improve lead capture, Podium launched AI Employee, their agentic application (and flagship product) to engage local business customers, schedule appointments, and close sales.\\xa0Initially, Podium used the LangChain framework for single-turn interactions. As their agentic use cases grew more complex for a wide-ranging set of customers and domains, Podium needed better visibility into their LLM calls and interactions — and turned to LangSmith for LLM testing and observability.Testing across the agentic development lifecycleEstablishing feedback loops was especially important to the agentic development lifecycle for Podium. LangSmith allowed the Podium engineers to test and continuously monitor their AI employee’s performance, adding new edge cases to their dataset to refine and test the model over time.Podium’s testing approach looks like the following:\\xa0\\xa0Baseline Dataset Curation: Create an initial dataset to represent basic use cases and requirements for the agent. This serves as a foundation for testing and development.Baseline Offline Evaluation: Conduct initial tests using the curated dataset to assess the agent's performance against the basic requirements before shipping to production.Collecting Feedback:\\xa0User-Provided Feedback: Collect direct input from users interacting with the agent.\\xa0Online Evaluation: Use LLMs to self-evaluate and monitor the quality of responses using in real-time, flagging potential issues for further investigation.Optimization:\\xa0Prompt Tuning: Refine the prompts used to guide the agent's responses.Retrieval Tuning: Adjust the retrieval mechanisms used to generate responses.Model Fine-Tuning: Use traced data to further train and specialize the model for specific tasks.Ongoing Evaluation:\\xa0Offline Evaluation: Evaluate the agent's performance and identify opportunities for optimization using backtesting, pairwise comparisons, and other testing methods.Dataset Curation: Continuously update and expand the test dataset with new scenarios and edge cases for regression testing, ensuring new changes don't negatively impact existing capabilities.How Podium creates testing loops for their agent Dataset curation and fine-tuning agents with LangSmithPrior to LangSmith, understanding a customer inquiry and what steps employees should take to resolve the inquiry was difficult, since the Podium engineers made 20-30 LLM calls per interaction. With LangSmith, they quickly got set up and logged and viewed traces to aggregate insights.One specific challenge Podium ran into with their AI Employee was that the agent struggled to recognize when a conversation had naturally ended, resulting in awkward repeated goodbyes. To address this, Podium began by creating a dataset in LangSmith with various conversation scenarios, including ways different conversations might conclude.\\xa0Their engineering team then found it helpful to upgrade to a larger model, curating the outputs into a smaller model (using a technique called model distillation). Upgrading their model went smoothly since model inputs and outputs were automatically captured in LangSmith’s traces, allowing the team to easily curate datasets.Podium engineers also enriched LangSmith traces with metadata on customer profiles, business types, and other parameters important to their business. They grouped traces using specific identifiers in LangSmith, making it easy to aggregate related traces during data curation. This enriched data enabled Podium to create a higher-quality and balanced dataset, which improved model fine-tuning and helped them avoid overfitting).\\xa0With this balanced dataset, the Podium team then compared the results from their fine-tuned model against results from their original, larger model using pairwise evaluations. This comparison allowed them to assess how well the upgraded model could improve the agent’s ability to know when to conclude a conversation.After fine-tuning, Podium’s new model showed significant improvement in detecting where natural conversation should end for its agent. Podium’s F1 scores with the fine-tune model experienced a 7.5% improvement, going from 91.7% to 98.6% to exceed their quality threshold of 98%.High-quality customer support for AI platform without engineering interventionAt Podium, engineers must understand when communications with customers go awry, so that they can keep shipping reliable and high-quality products.Since publicly launching their AI Employee in January, it became critical for the Technical Product Specialists (TPS) at Podium to troubleshoot issues users were encountering in real-time. At Podium, the TPS team typically provides customer support for their small business customers. However, pinpointing the source of issues (and how to take action on them) was challenging.\\xa0Giving the TPS team access to LangSmith provided clarity, allowing the team to quickly identify customer-reported issues and determine: “Is this issue caused by a bug in the application, incomplete context, misaligned instructions, or an issue with the LLM?”\\xa0For Podium, identifying the type of customer issue guided them to the appropriate interventions:For bugs in the application: These are orchestration failures, such as an integration failing to return data. These require engineering intervention.For incomplete context: LLM is missing information needed to answer a question. These can be remediated by the TPS team by adding additional content.For misaligned instructions: Instructions are based on business requirements; any issues in the requirements can affect agent behavior. These can be remediated by the TPS team making changes in the content authoring system to better suit business requirements.For an LLM issue: Even with necessary context, an LLM may produce unexpected or incorrect information. These require engineering intervention.For example, many car dealerships use Podium’s AI Employee to respond to customer inquiries. If the AI Employee mistakenly responds that a car dealership does not offer oil changes, the TPS team can use LangSmith’s playground feature to edit the system output and determine if a simple setting change in the Admin interface can resolve the issue.LangSmith Playground enables Podium’s support team to troubleshoot agent behavior without engineering interventionBefore LangSmith, troubleshooting agent behavior often required engineering intervention. This was a time-consuming process that involved calling in engineers to first review model inputs and outputs, and then rewrite and refactor the code.By giving their TPS team access to LangSmith traces, Podium has reduced the need for engineering intervention by 90%, allowing their engineers to focus more on development instead of support tasks.In summary, using LangSmith led to:\\xa0Increased efficiency of Podium’s support team by enabling them to resolve issues more quickly and independently.Improved customer satisfaction (CSAT) scores for both support interactions and Podium’s AI-powered services.What’s Next for PodiumBy integrating LangSmith and LangChain, Podium has gained a competitive edge in the space of customer experience tools. LangSmith has enhanced observability and simplified the management of large datasets and optimizing model performance. The Podium team has also been integrating LangGraph into its workflow, reducing complexity in their agent orchestration while serving different target customers, while increasing controllability over their agent conversations.\\xa0Together, these suite of products have allowed Podium to focus on their core value proposition — help small businesses capture leads more effectively —\\xa0and efficiently design, test, and monitor their LLM applications.Podium is hiring across roles to help local businesses win. Inspired by Podium’s story? You can also try out LangSmith for free or talk to a LangSmith expert to learn more.\\xa0And for a more comprehensive best practices for testing and evaluating your LLM application, check out this guidebook. \\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024\\n        \\n\\n\\n\\n\\n\\n\\n\"),\n",
              " Document(metadata={'source': 'https://blog.langchain.dev/langchain-integration-docs-revamped/', 'loc': 'https://blog.langchain.dev/langchain-integration-docs-revamped/', 'lastmod': '2024-08-14T15:37:12.000Z'}, page_content='\\n\\n\\nLangChain Integration Docs: Find information faster with revamped pages & API references\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integration Docs: Find information faster with revamped pages & API references\\nSee the latest updates to the LangChain integration docs, including a new standardized format and improved API references that can help you find relevant information faster.\\n\\n3 min read\\nAug 14, 2024\\n\\n\\n\\n\\n\\nA large part of the LangChain ecosystem is its extensive collection of integrations. LangChain offers over 1,000 integrations for LLMs, vector stores, tools, document loaders, and more.Today, we’re announcing an overhaul of our integration documentation in both Python and JavaScript to make it more useful and accessible for the community. The key changes include:A standardized format for all integration pages.A cleaned-up index page for each component, which includes a “Features” table highlighting which integrations support specific features.Improved API references that better surface examples and relevant information.Let’s dive in!Standardized content for all integration pagesOver the last year and a half, the LangChain community has contributed over 1,000 open-source integrations, including chat models, vector stores, tools, and retrievers. As the sheer quantity of integrations has grown and best practices have changed over time, many pages have become outdated.Key integrations now follow a standardized template that highlights common features for each category (e.g. models, vector stores, retrievers). For example, for chat models, each page begins with a table showing whether an integration supports features like tool calling and multimodal input, followed by installation and basic invocation examples.Our goal with these revamped integration pages is to help developers quickly identify what an integration can do and how to use it.Overview of standardized template for LangChain integration pagesWhile some advanced, integration-specific examples remain on these pages, we\\'ve placed more emphasis on linking to how-to guides and API references to keep the content evergreen and avoid repetition.New index pages for streamlined searchTo help developers find the integrations they want, we’ve also streamlined the index pages for each type of integration. Combined with the smaller sidebar, these index pages now include tables similar to those on individual integration pages, which lets you quickly identify an integration with the features you need.\"Features\" table in new index pages for each type of integrationThese “Features” tables are currently sorted by a combination of factors including usage in LangSmith traces and package downloads (where relevant). We’ll be looking into more ways to highlight and feature up-and-coming integrations in the future.Improved API referencesOur new pages prominently feature our improved API references for Python and JavaScript.For Python, we’ve added more explanations and usage examples into the docstrings. We’ve also updated the structure and formatting to be more modern and user-friendly, including a navigable sidebar of methods and attributes for all classes.LangChain Python API ReferenceFor JavaScript, we’ve gone in a similar direction. To make the API References pages less intimidating, we’ve collapsed the sidebar by default, filtering out less relevant methods and other build artifacts.We’ve also enhanced popular chat model and vector store pages with various usage examples, and have generally improved the visibility of useful runtime and constructor definitions and important methods on these pages.LangChain JavaScript API ReferenceThis ongoing work aims to make our API references stand on their own as valuable resources to the LangChain community.Check out our latest integration docs for Python and for JavaScript. Your feedback is invaluable as we continue to refine and improve our documentation. Feel free to drop us a line on Twitter with any questions, suggestions, or comments.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024\\n        \\n\\n\\n\\n\\n\\n\\n'),\n",
              " Document(metadata={'source': 'https://blog.langchain.dev/langgraph-v0-2/', 'loc': 'https://blog.langchain.dev/langgraph-v0-2/', 'lastmod': '2024-08-12T14:45:47.000Z'}, page_content=\"\\n\\n\\nLangGraph v0.2: Increased customization with new checkpointer libraries\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph v0.2: Increased customization with new checkpointer libraries\\nLangGraph v0.2 includes new checkpointer libraries for increased customization —\\xa0including a SQLite checkpointer for local workflows and an optimized Postgres checkpointer to take your app to production. Plus, learn about LangGraph Cloud in open beta. \\n\\n4 min read\\nAug 7, 2024\\n\\n\\n\\n\\n\\nToday, we’re excited to announce the stable release of LangGraph v0.2, which introduces a new ecosystem of LangGraph checkpointer libraries. These simplify the creation and customization of checkpointers, which allows users to build more resilient LLM applications with smooth session memory, robust error recovery, and human-in-the-loop features.Why we built LangGraph v0.2One of the key pillars of LangGraph is its built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at each step, enabling several powerful capabilities, including:Session memory: Store history (checkpoints) of user interactions and resume from a saved checkpoint in follow up interactionsError recovery: Recover from failures at any given step in the graph execution by continuing from the last successful step checkpointHuman-in-the-loop: Implement tool approval, wait for human input, edit agent actions and moreTime travel: Edit graph state at any point in the history of execution and create an alternative execution from that point in time (i.e. fork the thread)Since the early days of LangGraph, we’ve designed checkpointing to be database-agnostic, allowing users to implement their own checkpointer adapters for their database of choice.Since the LangGraph v0.1 release, we've seen a lot of interest from the community in creating checkpointers for many popular databases like Postgres, Redis, and MongoDB. However, there was no clear blueprint for the users to implement their own, custom checkpointers.New checkpointer libraries in LangGraphWith LangGraph v0.2, we’re making it easier to create new checkpointers. We’ve also laid the foundation to foster a community-maintained ecosystem of checkpointer implementations.We now have a suite of new, dedicated checkpointer libraries:langgraph_checkpoint : The base interface for checkpointer savers (BaseCheckpointSaver ) and serialization/deserialization interface (SerializationProtocol). Includes in-memory checkpointer implementation (MemorySaver) for experimentation.langgraph_checkpoint_sqlite : An implementation of LangGraph checkpointer that uses SQLite database. Ideal for experimentation and local workflows.langgraph_checkpoint_postgres : Our advanced checkpointer that we wrote and optimized for Postgres in LangGraph Cloud is now open-sourced and available to the community to build upon. Ideal for using in production.Checkpointer implementations can be used interchangeably, which lets users tailor their stateful LangGraph applications to their custom needs.LangGraph Postgres Checkpointer for production-ready appslanggraph_checkpoint_postgres implementation can serve as a blueprint for community members to implement their own optimized, production-ready checkpointers for their favorite database. Postgres checkpointer implements a number of optimizations both on the write-, as well as read-side.Write-side optimizations:We're making use of Postgres pipeline mode to reduce database roundtripsWe're storing each channel value separately and versioned so that each new checkpoint only stores the values that changed.Read-side optimizations:We're making use of a cursor for the list endpoint in order to efficiently fetch long thread histories when needed.Getting started on LangGraph v0.2Since LangGraph checkpointer libraries are implemented as namespace packages, you can import checkpointer interfaces and implementations the same way as before, using:from langgraph.checkpoint.base import BaseCheckpointSaverfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.checkpoint.sqlite import SqliteSaverfrom langgraph.checkpoint.postgres import PostgresSaverSince SQLite and Postgres checkpointers are provided via separate libraries, you will need to install them using pip install langgraph-checkpoint-sqlite or pip install langgraph-checkpoint-postgres.LangGraph checkpoint libraries will follow semantic versioning (starting with current release of 1.0), and any breaking changes in the main library will result in a major version bump for those libraries. For example, the next breaking change in langgraph_checkpoint will result in 2.0 version, and you can expect the checkpointer implementations (e.g.,langgraph_checkpoint_sqlite) to also be updated to 2.0 to follow that change.To get started, follow this guide on how to use checkpointers in LangGraph. You can also check out our documentation, including a reference and overview of checkpointers. Run agents at scale with LangGraph CloudTo complement the LangGraph framework, we also have a new runtime, LangGraph Cloud, which provides infrastructure purpose-built for deploying agents at scale.LangGraph Cloud does the heavy lifting for your agentic application, removing the maintenance work for custom checkpointers while adding fault-tolerant scalability. It gracefully manages horizontally-scaling task queues, servers, and includes our robust Postgres checkpointer out-of-the-box to help you handle many concurrent users and efficiently store large states and threads.In addition, LangGraph Cloud supports real-world interaction patterns beyond streaming and human-in-the-loop. These include double-texting to handle new user inputs on currently-running threads of the graph, async background jobs for long-running tasks, and cron jobs.Lastly, you can easily deploy your agentic app and collaborate in LangGraph Studio, a playground-like space for visualizing and debugging agent trajectories, with LangGraph Cloud. The LangGraph Studio desktop app is now also available for all LangSmith users to try for free.LangGraph Cloud is now available in beta for all LangSmith users on Plus or Enterprise plans. Try it out today for free by signing up for LangSmith.Additional changes in LangGraph v0.2LangGraph v0.2 contains many improvements, and we've designed it to be largely backwards compatible. Below is a list of breaking changes and deprecations in this latest version.Breaking changesLangGraph v0.2 introduces several breaking changes:thread_ts and parent_ts have been renamed to checkpoint_id and parent_checkpoint_id , respectively (via langgraph_checkpoint==1.0.0).Note: LangGraph checkpointers still recognize thread_ts if passed via config and treat it as checkpoint_idRe-exported imports like from langgraph.checkpoint import BaseCheckpointSaver are no longer possible due to the use of namespace packages. Instead, use from langgraph.checkpoint.base import BaseCheckpointSaverSQLite checkpointers have been moved to a separate library, so you’ll need to install them separately using pip install langgraph-checkpoint-sqliteDeprecationsIn LangGraph v0.2, we've removed:langgraph.prebuilt.chat_agent_executor.create_function_calling_executor . We recommend you use langgraph.prebuilt.chat_agent_executor.create_react_agent instead.langgraph.prebuilt.agent_executor . We recommend you use langgraph.prebuilt.chat_agent_executor.create_react_agent instead.ConclusionWe are incredibly grateful to our community and users for pushing us and building with LangGraph to improve agent reliability. We hope that with LangGraph v0.2, you’ll find it easier to build and maintain your own checkpointer implementations– and we’re excited to see all the apps that you create.As you try out LangGraph v0.2, we'd love to hear your feedback at hello@langchain.dev. You can also learn more from these additional resources:LangGraph docsLangGraph webpage (with FAQs)\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024\\n        \\n\\n\\n\\n\\n\\n\\n\"),\n",
              " Document(metadata={'source': 'https://blog.langchain.dev/dynamic-few-shot-examples-langsmith-datasets/', 'loc': 'https://blog.langchain.dev/dynamic-few-shot-examples-langsmith-datasets/', 'lastmod': '2024-08-11T23:50:33.000Z'}, page_content='\\n\\n\\nDynamic few-shot examples with LangSmith datasets\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDynamic few-shot examples with LangSmith datasets\\nWith dynamic few-shot examples in LangSmith, you can Index examples in your datasets in one click and dynamically select the most relevant few-shot examples based on user input. This lets you rapidly iterate and improve LLM app performance.\\n\\n\\n3 min read\\nAug 6, 2024\\n\\n\\n\\n\\n\\nToday, we are launching dynamic few-shot example selectors as part of LangSmith. Few shot prompting is a common technique used to improve application performance. Dynamically selecting the examples for a few-shot prompt can yield further improvements. To do this, you need the ability to curate a dataset and to have the infrastructure to index and search over said dataset.LangSmith already easily enables dataset curation. Now, with this new feature, you can index examples in your datasets with the click of a button. This lets you dynamically select the most relevant few-shot examples based on user input, allowing you to refine and improve LLM apps more efficiently.The challenges of optimizing model performanceFew-shot prompting involves including example model inputs and desired outputs in the model prompt, which can greatly boost model performance on a wide range of tasks. Typically, developers use 3-5 few shot examples in a prompt to avoid overloading the context window with trivial information.As your application grows in complexity, however, you may need hundreds or even thousands of examples to cover diverse end user needs. Including a large dataset of examples in every request may be too large to add to the context window. Even if it fits, it will increase token cost and latency.When working with this many examples, finetuning is often the next option. But while finetuning can be powerful and yield good results if done properly, it also has some downsides. Finetuning is (a) relatively complex to execute, (b) hard to update with new examples, (c) requires specialized infrastructure (GPUs) and expertise (MLEs), and (d) lacks personalization (i.e. can’t easily tailor examples to users).As a result, finetuning may be less suitable for personalized applications and rapid iterations. This is where dynamic few-shot prompting can come in handy.Dynamic few-shot examples in LangSmithDynamic few-shot prompting can help you rapidly iterate and improve LLM application performance. With this technique, you still use a small set of 3-5 examples but dynamically select which examples to use based on user input. These examples cover the full range of options, which can outperform static datasets of 3-5 examples. The difference between “dynamic” and “static” few-shot prompting is that with “dynamic” prompting, you select examples to include in the prompt based on user input, while with “static” prompting, the same examples are used regardless of user input.We’ve integrated dynamic few-shot prompting into LangSmith to streamline dataset management and enhance your LLM application performance. With just one click, you can index your dataset and call an endpoint with a new input to get back a list of examples most similar to the new input.Compared to fine-tuning, dynamic few-shot prompting is (a) much easier to do technically, (b) easier to keep up-to-date, and (c) requires minimal specialized infrastructure. You can retrieve relevant examples based on user inputs, making it easy to iterate quickly on applications that you can also adapt and personalize.The new button we added in LangSmith that allows you to index a dataset as a few-shot datasetWith dynamic few-shot prompting, you can streamline dataset management and refine your LLM app performance in LangSmith. Learn more on how to use dynamic few-shot prompting in our detailed technical documentation. You can also watch our video walkthrough below.Dynamic few-shot prompting in LangSmith is currently in closed beta. You can sign up for the waitlist here. We are targeting a public launch later this month.To start building a data flywheel to improve your LLM application, check out our other resources on managing data and testing in LangSmith, including:Curating a datasetSelf-improving evaluation\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024\\n        \\n\\n\\n\\n\\n\\n\\n'),\n",
              " Document(metadata={'source': 'https://blog.langchain.dev/ux-for-agents-part-3/', 'loc': 'https://blog.langchain.dev/ux-for-agents-part-3/', 'lastmod': '2024-08-10T14:00:35.000Z'}, page_content='\\n\\n\\nUX for Agents, Part 3: Spreadsheet, Generative, and Collaborative UI/UX\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUX for Agents, Part 3: Spreadsheet, Generative, and Collaborative UI/UX\\nLearn about spreadsheet UX for batch agent workloads, Generative UI, and collaborative UX with agents.\\n\\nIn the Loop\\n4 min read\\nAug 10, 2024\\n\\n\\n\\n\\n\\nAt Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. Check out that talk here. Since UX for agents is such a wide-ranging topic, we’ve split our discussion of it into three posts. See the first blog post on chat UX and the second on ambient UX. This is our third post on UX for agents, focused on spreadsheet, generative, and collaborative UI/UX.This is my third post on agent UX, but I could probably dive into ten more — there is so much to explore as we all figure out the best ways to build and interact with agents. The UI/UX space for agents is one of the spaces I am most excited about and will be watching closely for innovation in the coming months.In an attempt to wrap up the discussion on agent UI/UX, I’ll highlight three lesser-known UXs that have recently become more popular. Each of these could easily deserve its own blog post (which may happen down the line!).Spreadsheet UXOne UX paradigm I’ve seen a lot in the past ~2 months is a spreadsheet UX. I first saw this when Matrices, an AI-native spreadsheet, was launched earlier this year.I loved seeing this. First and foremost, the spreadsheet UX a super intuitive and user friendly way to support batch workloads. Each cell becomes it own agent, going to off to research a particular thing. This batching allows users to scale up and interact with multiple agents simultaneously.There are other benefits of this UX as well. The spreadsheet format is a very common UX familiar to most users, so it fits in well with existing workflows. This type of UX is perfect for data enrichment, a common LLM use case where each column can represent a different attribute to enrich.Since then, I’ve seen this UX pop up in a few places (Clay and Otto are two great examples of this).Generative UIThe concept of “generative UI” can mean a few different things.One interpretation is a truly generative UI where the model generates the raw components to display. This is similar to applications like WebSim. Under the hood, the agent is largely writing raw HTML, allowing it to have FULL control over what is displayed. However, this approach allows for a lot of variability in the quality of the generated HTML, so the end result may look a bit wild or unpolished.Another more constrained approach to generative UI involves programmatically mapping the LLM response to different pre-defined UI components. This is often done with tool calls. For instance, if an LLM calls a weather API, it then triggers the rendering of a weather map UI component. Since the components rendered are not truly generated (but more chosen), the resulting UI will be more polished, though less flexible in what it can generate.You can learn more about generative UI in our video series here.Collaborative UXA lesser explored UX: what happens when agents and humans are working together? Think Google Docs, where you can collaborate with teammates on writing or editing documents - but instead, one of your collaborators is an agent.The leading thinkers in the space in my mind are Geoffrey Litt and Ink & Switch, with their Patchwork project being a great example of human-agent collaboration.How does collaborative UX compare to the previously discussed ambient UX? Our founding engineer Nuno highlights the key differences between the two:The main difference between ambient and collaborative is concurrency:In a collaborative UX, you and the LLM often do work simultaneously, \"feeding\" off of each others workIn an ambient UX, the LLM is continuously doing work in the background while you, the user, focus on something else entirelyThese differences also translate into distinct requirements when building these applications:For collaborative UX, you may need to display granular pieces of work being done by the LLM. (This falls somewhere on the spectrum between individual tokens and larger, application-specific pieces of work like paragraphs in a text editor). A common requirement might be having an automated way to merge concurrent changes, similar to how Google Doc manages real-time collaboration.For ambient UX, you may need to summarize the work done by the LLM or highlight any changes. A common requirement might be to trigger a run from an event that happened in some other system, e.g. via a webhook.Why are we thinking about this?LangChain is not known for being a UI/UX focused company. But we spend a lot of time thinking about this. Why?Our goal is to make it as easy as possible to build agentic applications. How humans interact with these applications greatly affects the type of infrastructure that we need to build.For example - we recently launched LangGraph Cloud, our infrastructure for deploying agentic applications at scale. It features multiple streaming modes, support for “double-texting” use cases, and async background runs. All of these were directly influenced by UI/UX trends that we saw emerging.If you are building an application with a novel or interesting UI/UX (e.g. non-streaming chat) we would love to hear from you at hello@langchain.dev!\\n\\n\\nTags\\nIn the Loop\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUX for Agents, Part 2: Ambient\\n\\n\\nIn the Loop\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUX for Agents, Part 1: Chat\\n\\n\\nIn the Loop\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlanning for Agents\\n\\n\\nIn the Loop\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhy you should outsource your agentic infrastructure, but own your cognitive architecture\\n\\n\\nHarrison Chase\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is a \"cognitive architecture\"?\\n\\n\\nHarrison Chase\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is an agent?\\n\\n\\nHarrison Chase\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024\\n        \\n\\n\\n\\n\\n\\n\\n')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "a666d551-6b20-4761-82ad-6f9b524d1660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4821"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "from aimakerspace.openai_utils.embedding import EmbeddingModel\n",
        "from aimakerspace.semantic_chunking import SemanticChunking\n",
        "from aimakerspace.pdfloader import PDFLoader\n",
        "import pprint\n",
        "\n",
        "chat_openai = ChatOpenAI(model_name=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_model = EmbeddingModel()\n",
        "chunker = SemanticChunking(embedding_model, breakpoint_threshold_type=\"percentile\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_pages = []\n",
        "for doc in documents:\n",
        "    split_pages += chunker.split_text(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_documents = [i.page_content for i in split_pages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"\\n\\n\\nHow Podium optimized agent behavior and reduced engineering intervention by 90% with LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Podium optimized agent behavior and reduced engineering intervention by 90% with LangSmith\\nSee how Podium tests across the lifecycle development of their AI employee agent, using LangSmith for dataset curation and finetuning. They improved agent F1 response quality to 98% and reduced the need for engineering intervention by 90%. 5 min read\\nAug 15, 2024\\n\\n\\n\\n\\n\\nAbout PodiumPodium is a communication platform that helps small businesses connect quickly with customers via phone, text, email, and social media. Small businesses often have high-touch interactions with customers — think automotive dealers, jewelers, bike shops — yet are understaffed. Podium's mission is to help these businesses respond to customer inquiries promptly so that they can convert leads into sales. Podium data shows that responding to customer inquiries within 5 minutes results in a 46% higher lead conversion rate than responding in an hour. To improve lead capture, Podium launched AI Employee, their agentic application (and flagship product) to engage local business customers, schedule appointments, and close sales. Initially, Podium used the LangChain framework for single-turn interactions. As their agentic use cases grew more complex for a wide-ranging set of customers and domains, Podium needed better visibility into their LLM calls and interactions — and turned to LangSmith for LLM testing and observability.Testing across the agentic development lifecycleEstablishing feedback loops was especially important to the agentic development lifecycle for Podium. LangSmith allowed the Podium engineers to test and continuously monitor their AI employee’s performance, adding new edge cases to their dataset to refine and test the model over time.Podium’s testing approach looks like the following:\\xa0\\xa0Baseline Dataset Curation: Create an initial dataset to represent basic use cases and requirements for the agent. This serves as a foundation for testing and development.Baseline Offline Evaluation: Conduct initial tests using the curated dataset to assess the agent's performance against the basic requirements before shipping to production.Collecting Feedback:\\xa0User-Provided Feedback: Collect direct input from users interacting with the agent. Online Evaluation: Use LLMs to self-evaluate and monitor the quality of responses using in real-time, flagging potential issues for further investigation.Optimization:\\xa0Prompt Tuning: Refine the prompts used to guide the agent's responses.Retrieval Tuning: Adjust the retrieval mechanisms used to generate responses.Model Fine-Tuning: Use traced data to further train and specialize the model for specific tasks.Ongoing Evaluation:\\xa0Offline Evaluation: Evaluate the agent's performance and identify opportunities for optimization using backtesting, pairwise comparisons, and other testing methods.Dataset Curation: Continuously update and expand the test dataset with new scenarios and edge cases for regression testing, ensuring new changes don't negatively impact existing capabilities.How Podium creates testing loops for their agent Dataset curation and fine-tuning agents with LangSmithPrior to LangSmith, understanding a customer inquiry and what steps employees should take to resolve the inquiry was difficult, since the Podium engineers made 20-30 LLM calls per interaction. With LangSmith, they quickly got set up and logged and viewed traces to aggregate insights.One specific challenge Podium ran into with their AI Employee was that the agent struggled to recognize when a conversation had naturally ended, resulting in awkward repeated goodbyes. To address this, Podium began by creating a dataset in LangSmith with various conversation scenarios, including ways different conversations might conclude. Their engineering team then found it helpful to upgrade to a larger model, curating the outputs into a smaller model (using a technique called model distillation). Upgrading their model went smoothly since model inputs and outputs were automatically captured in LangSmith’s traces, allowing the team to easily curate datasets.Podium engineers also enriched LangSmith traces with metadata on customer profiles, business types, and other parameters important to their business. They grouped traces using specific identifiers in LangSmith, making it easy to aggregate related traces during data curation. This enriched data enabled Podium to create a higher-quality and balanced dataset, which improved model fine-tuning and helped them avoid overfitting). With this balanced dataset, the Podium team then compared the results from their fine-tuned model against results from their original, larger model using pairwise evaluations. This comparison allowed them to assess how well the upgraded model could improve the agent’s ability to know when to conclude a conversation.After fine-tuning, Podium’s new model showed significant improvement in detecting where natural conversation should end for its agent. Podium’s F1 scores with the fine-tune model experienced a 7.5% improvement, going from 91.7% to 98.6% to exceed their quality threshold of 98%.High-quality customer support for AI platform without engineering interventionAt Podium, engineers must understand when communications with customers go awry, so that they can keep shipping reliable and high-quality products.Since publicly launching their AI Employee in January, it became critical for the Technical Product Specialists (TPS) at Podium to troubleshoot issues users were encountering in real-time. At Podium, the TPS team typically provides customer support for their small business customers. However, pinpointing the source of issues (and how to take action on them) was challenging. Giving the TPS team access to LangSmith provided clarity, allowing the team to quickly identify customer-reported issues and determine: “Is this issue caused by a bug in the application, incomplete context, misaligned instructions, or an issue with the LLM?”\\xa0For Podium, identifying the type of customer issue guided them to the appropriate interventions:For bugs in the application: These are orchestration failures, such as an integration failing to return data. These require engineering intervention.For incomplete context: LLM is missing information needed to answer a question. These can be remediated by the TPS team by adding additional content.For misaligned instructions: Instructions are based on business requirements; any issues in the requirements can affect agent behavior. These can be remediated by the TPS team making changes in the content authoring system to better suit business requirements.For an LLM issue: Even with necessary context, an LLM may produce unexpected or incorrect information. These require engineering intervention.For example, many car dealerships use Podium’s AI Employee to respond to customer inquiries. If the AI Employee mistakenly responds that a car dealership does not offer oil changes, the TPS team can use LangSmith’s playground feature to edit the system output and determine if a simple setting change in the Admin interface can resolve the issue.LangSmith Playground enables Podium’s support team to troubleshoot agent behavior without engineering interventionBefore LangSmith, troubleshooting agent behavior often required engineering intervention. This was a time-consuming process that involved calling in engineers to first review model inputs and outputs, and then rewrite and refactor the code.By giving their TPS team access to LangSmith traces, Podium has reduced the need for engineering intervention by 90%, allowing their engineers to focus more on development instead of support tasks.In summary, using LangSmith led to:\\xa0Increased efficiency of Podium’s support team by enabling them to resolve issues more quickly and independently.Improved customer satisfaction (CSAT) scores for both support interactions and Podium’s AI-powered services.What’s Next for PodiumBy integrating LangSmith and LangChain, Podium has gained a competitive edge in the space of customer experience tools. LangSmith has enhanced observability and simplified the management of large datasets and optimizing model performance. The Podium team has also been integrating LangGraph into its workflow, reducing complexity in their agent orchestration while serving different target customers, while increasing controllability over their agent conversations. Together, these suite of products have allowed Podium to focus on their core value proposition — help small businesses capture leads more effectively —\\xa0and efficiently design, test, and monitor their LLM applications.Podium is hiring across roles to help local businesses win. Inspired by Podium’s story? You can also try out LangSmith for free or talk to a LangSmith expert to learn more. And for a more comprehensive best practices for testing and evaluating your LLM application, check out this guidebook. Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application... Success!\",\n",
              " 'Please check your inbox and click the link to confirm your subscription. Sorry, something went wrong.',\n",
              " 'Please try again. Sign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024\\n        \\n\\n\\n\\n\\n\\n\\n',\n",
              " '\\n\\n\\nLangChain Integration Docs: Find information faster with revamped pages & API references\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integration Docs: Find information faster with revamped pages & API references\\nSee the latest updates to the LangChain integration docs, including a new standardized format and improved API references that can help you find relevant information faster. 3 min read\\nAug 14, 2024\\n\\n\\n\\n\\n\\nA large part of the LangChain ecosystem is its extensive collection of integrations. LangChain offers over 1,000 integrations for LLMs, vector stores, tools, document loaders, and more.Today, we’re announcing an overhaul of our integration documentation in both Python and JavaScript to make it more useful and accessible for the community. The key changes include:A standardized format for all integration pages.A cleaned-up index page for each component, which includes a “Features” table highlighting which integrations support specific features.Improved API references that better surface examples and relevant information.Let’s dive in!Standardized content for all integration pagesOver the last year and a half, the LangChain community has contributed over 1,000 open-source integrations, including chat models, vector stores, tools, and retrievers. As the sheer quantity of integrations has grown and best practices have changed over time, many pages have become outdated.Key integrations now follow a standardized template that highlights common features for each category (e.g. models, vector stores, retrievers). For example, for chat models, each page begins with a table showing whether an integration supports features like tool calling and multimodal input, followed by installation and basic invocation examples.Our goal with these revamped integration pages is to help developers quickly identify what an integration can do and how to use it.Overview of standardized template for LangChain integration pagesWhile some advanced, integration-specific examples remain on these pages, we\\'ve placed more emphasis on linking to how-to guides and API references to keep the content evergreen and avoid repetition.New index pages for streamlined searchTo help developers find the integrations they want, we’ve also streamlined the index pages for each type of integration. Combined with the smaller sidebar, these index pages now include tables similar to those on individual integration pages, which lets you quickly identify an integration with the features you need.\"Features\" table in new index pages for each type of integrationThese “Features” tables are currently sorted by a combination of factors including usage in LangSmith traces and package downloads (where relevant). We’ll be looking into more ways to highlight and feature up-and-coming integrations in the future.Improved API referencesOur new pages prominently feature our improved API references for Python and JavaScript.For Python, we’ve added more explanations and usage examples into the docstrings. We’ve also updated the structure and formatting to be more modern and user-friendly, including a navigable sidebar of methods and attributes for all classes.LangChain Python API ReferenceFor JavaScript, we’ve gone in a similar direction. To make the API References pages less intimidating, we’ve collapsed the sidebar by default, filtering out less relevant methods and other build artifacts.We’ve also enhanced popular chat model and vector store pages with various usage examples, and have generally improved the visibility of useful runtime and constructor definitions and important methods on these pages.LangChain JavaScript API ReferenceThis ongoing work aims to make our API references stand on their own as valuable resources to the LangChain community.Check out our latest integration docs for Python and for JavaScript. Your feedback is invaluable as we continue to refine and improve our documentation. Feel free to drop us a line on Twitter with any questions, suggestions, or comments. Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application... Success! Please check your inbox and click the link to confirm your subscription. Sorry, something went wrong.',\n",
              " 'Please try again. Sign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024\\n        \\n\\n\\n\\n\\n\\n\\n']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Alright, now we have 516 ~200 token long documents.\n",
        "\n",
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "9813bfb9-cf80-4787-c962-69a5b1ccdff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "499\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in split_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26224\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in split_pages:\n",
        "    max_chunk_length = max(max_chunk_length, len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model2 = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    documents=split_pages, embedding=embedding_model2, location=\":memory:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    documents=split_chunks, embedding=embedding_model, location=\":memory:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the LCEL we touched on earlier to create a RAG chain.\n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "####🏗️ Activity #2:\n",
        "\n",
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\n",
        "Use the provided context to answer the user's query.\n",
        "\n",
        "You may not answer the user's query unless there is specific context in the following text.\n",
        "\n",
        "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2tNXIT1iuB"
      },
      "source": [
        "We'll set our Generator - `gpt-4o` in this case - below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rZ-9gF1x1iEz"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"base_llm\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Chain\n",
        "\n",
        "Notice how we have a bit of a more complex chain this time - that's because we want to return our sources with the response.\n",
        "\n",
        "Let's break down the chain step-by-step:\n",
        "\n",
        "1. We invoke the chain with the `question` item. Notice how we only need to provide `question` since both the retreiver and the `\"question\"` object depend on it.\n",
        "  - We also chain our `\"question\"` into our `retriever`! This is what ultimately collects the context through Qdrant.\n",
        "2. We assign our collected context to a `RunnablePassthrough()` from the previous object. This is going to let us simply pass it through to the next step, but still allow us to run that section of the chain.\n",
        "3. We finally collect our response by chaining our prompt, which expects both a `\"question\"` and `\"context\"`, into our `llm`. We also, collect the `\"context\"` again so we can output it in the final response object.\n",
        "\n",
        "The key thing to keep in mind here is that we need to pass our context through *after* we've retrieved it - to populate the object in a way that doesn't require us to call it or try and use it for something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceq3JfVLM74-",
        "outputId": "4235f7af-4430-4c08-d94d-99e482ef30af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "eb34fc03-7052-4825-82e1-85489ac84e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+      \n",
            "          | Parallel<context,question>Input |      \n",
            "          +---------------------------------+      \n",
            "                    **            **               \n",
            "                  **                **             \n",
            "                **                    **           \n",
            "         +--------+                     **         \n",
            "         | Lambda |                      *         \n",
            "         +--------+                      *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "  +----------------------+          +--------+     \n",
            "  | VectorStoreRetriever |          | Lambda |     \n",
            "  +----------------------+          +--------+     \n",
            "                    **            **               \n",
            "                      **        **                 \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<context,question>Output |     \n",
            "          +----------------------------------+     \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "              +------------------------+           \n",
            "              | Parallel<context>Input |           \n",
            "              +------------------------+           \n",
            "                     ***        ***                \n",
            "                    *              *               \n",
            "                  **                **             \n",
            "           +--------+          +-------------+     \n",
            "           | Lambda |          | Passthrough |     \n",
            "           +--------+          +-------------+     \n",
            "                     ***        ***                \n",
            "                        *      *                   \n",
            "                         **  **                    \n",
            "              +-------------------------+          \n",
            "              | Parallel<context>Output |          \n",
            "              +-------------------------+          \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "          +---------------------------------+      \n",
            "          | Parallel<response,context>Input |      \n",
            "          +---------------------------------+      \n",
            "                   **              ***             \n",
            "                ***                   **           \n",
            "              **                        ***        \n",
            "+--------------------+                     **      \n",
            "| ChatPromptTemplate |                      *      \n",
            "+--------------------+                      *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "    +------------+                     +--------+  \n",
            "    | ChatOpenAI |                     | Lambda |  \n",
            "    +------------+*                  **+--------+  \n",
            "                   **              **              \n",
            "                     ***        ***                \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<response,context>Output |     \n",
            "          +----------------------------------+     \n"
          ]
        }
      ],
      "source": [
        "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQVzN_eX1M2"
      },
      "source": [
        "Let's try another visual representation:\n",
        "\n",
        "![image](https://i.imgur.com/Ad31AhL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What's new in LangChain v0.2?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "yfEAoG3HLC3J",
        "outputId": "e1633e4f-5405-46c3-f344-ffc324ac69d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('LangChain v0.2 is a pre-release that builds upon the foundation laid in '\n",
            " 'v0.1, focusing on improving stability and security. Key differences '\n",
            " 'include:\\n'\n",
            " '\\n'\n",
            " '1. **Separation of Packages**: LangChain v0.2 introduces a full separation '\n",
            " 'of the langchain package from langchain-community, making langchain more '\n",
            " 'lightweight and secure. Langchain-community now depends on langchain-core '\n",
            " 'and langchain.\\n'\n",
            " '\\n'\n",
            " '2. **Versioned Documentation**: The documentation for LangChain has been '\n",
            " 'revamped to be versioned, addressing community feedback. This allows users '\n",
            " 'to access both v0.1 and v0.2 documentation.\\n'\n",
            " '\\n'\n",
            " '3. **Agent Framework**: The agent framework has been made more mature and '\n",
            " 'controllable. LangGraph, an extension aimed at creating agentic workloads, '\n",
            " 'becomes the recommended way to build agents in v0.2, while the old '\n",
            " 'AgentExecutor is still available.\\n'\n",
            " '\\n'\n",
            " '4. **Standardization of LLM Interface**: There is improved standardization '\n",
            " 'around tool calling and the chat model interface, making it easier to switch '\n",
            " 'between different LLMs.\\n'\n",
            " '\\n'\n",
            " '5. **Streaming Support**: Enhanced streaming support has been implemented, '\n",
            " 'crucial for LLM applications.\\n'\n",
            " '\\n'\n",
            " '6. **Community Feedback Integration**: The v0.2 release incorporates various '\n",
            " 'improvements based on community feedback received since the release of '\n",
            " 'v0.1.\\n'\n",
            " '\\n'\n",
            " 'Overall, v0.2 represents a significant step towards stability, usability, '\n",
            " 'and community engagement compared to v0.1.')\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "response = retrieval_augmented_qa_chain.invoke(\n",
        "    {\n",
        "        \"question\": \"Can you tell me about Langchain v0.1 vs v0.2?\"\n",
        "    }\n",
        ")\n",
        "pprint.pprint(response[\"response\"].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "FSZFdCM5LFoq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "page_content='\n",
            "\n",
            "\n",
            "LangChain v0.2: A Leap Towards Stability\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain v0.2: A Leap Towards Stability\n",
            "Today, we're announcing the pre-release of LangChain v0.2, which improves the stability and security of LangChain. 5 min read\n",
            "May 10, 2024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Four months ago, we released the first stable version of LangChain. Today, we are following up by announcing a pre-release of langchain v0.2.This release builds upon the foundation laid in v0.1 and incorporates community feedback. We’re excited to share that v0.2 brings: A much-desired full separation of langchain and langchain-community New (and versioned!) docs A more mature and controllable agent framework Improved LLM interface standardization, particularly around tool callingBetter streaming support30+ new partner packages.This is just a pre-release, with the full v0.2 release coming in a few weeks. Let’s dive into what langchain v0.2 will include.Embracing stability: The evolution of LangChain architectureOne of the most notable changes in langchain v0.2 is the decoupling of the langchain package from langchain-community. As a result, langchain-community now depends on langchain-core and langchain. This is a continuation of the work we began with langchain v0.1.0 to create a more robust and self-contained package. 💡As a reminder, langchain v0.1.0 broke down the langchain package into component packages to improve the usability of LangChain in production environments. These included langchain-core, langchain, langchain-community, and partner packages. Read here to learn more.langchain-community contains a lot of third party integrations. This means there are a lot of (optional) dependencies, a lot of files, and due to the nature of the integrations, the package is occasionally vulnerable to CVEs. Hence, removing the dependency of langchain on langchain-community makes langchain more lightweight, more focused, and more secure.We tried to do this reorganization in a minimally disruptive way by continuing to expose the pre-existing entry points, which can be accessed from langchain.chat_models import ChatOpenAI. Under the hood, this involves a conditional import from langchain_community; essentially, it checks if langchain-community is installed, and if so, it does the import. This means that if you have langchain-community installed, there will be no breaking changes. This idea came from a community member - thanks Jacob!Improving discoverability: Better versioned documentationWe’ve also revamped our documentation based on community feedback. This effort started over a month ago, and we’re continuing in two main ways.First, documentation will now be versioned. This has been a constant community request, and we’ve worked hard to make this a reality. We will maintain the existing documentation as a v0.1 build, and start building a separate v0.2 build. For now the documentation will default to v0.1 — but once the full 0.2 release is out, we will begin to default to the new documentation. Our versioned docs should better reflect the state of the package, and we hope to only improve from here.Second, documentation is now more flat and simple. There are four main sections: tutorials, how-to guides, conceptual guides, and API reference. This should make it easier to find documentation, and for us to keep a minimal set of consistent guides. This was also a community suggestion - thank you Reddit user @Zealousideal_Wolf624!A suggestion on Reddit for the LangChain documentationWe’re also working on a “LangChain over time” documentation page to better highlight changes to LangChain. We hope this helps assist in explaining and relating concepts across versions.Increasing power: LangGraph agentsSince the early days of LangChain, one of the biggest points of feedback has been that it’s tough to customize the internals of pre-built chains and agents. We introduced LCEL last summer to solve that for chains, making it easy to create arbitrary composable sequences.Up until this point, agents in LangChain have always been based around AgentExecutor, a single class with hard coded logic for how to run an agent. We’ve added more and more arguments to this class to support increasingly advanced agents, but it’s still not truly composable.A few months ago we introduced LangGraph, an extension of LangChain specifically aimed at creating agentic workloads. Think of it as an “LCEL for agents”. It builds on top of LCEL adding in two important components: the ability to easily define cycles (important for agents, but not needed for chains) and built-in memory. In langchain v0.2, we are keeping the old AgentExecutor — but LangGraph is becoming the recommended way to build agents. We’ve added a prebuilt LangGraph object that is equivalent to AgentExecutor — which, by being built on LangGraph, is far easier to customize and modify. See here for documentation on how to migrate.Evolving v0.1.0: Improved support for streaming, standardized tool calling, and more Since the release of langchain v0.1.0 earlier this year in January, we’ve made sizable improvements in the following areas:Standard Chat Model Interface: We want to make it as easy as possible to switch seamlessly between different LLMs. In order to do this, we’ve standardized tool calling support as well as added a standardized interface for structuring output.' metadata={'_id': '95ea775db1e54ae8b122a824c08bdbf5', '_collection_name': '3f393dc3884e41518f548a249e5f53e9'}\n",
            "----\n",
            "Context:\n",
            "page_content='Async Support: We’ve improved our async support for many core abstractions. Here’s an example or two. Huge thanks and shout out to @cbornet for helping make this a reality!Streaming Support: Streaming is crucial for LLM applications, and we’ve improved our streaming support by adding in an Event Streaming API.Partner Packages: Having stable and reliable integrations is a top priority for us. We’ve worked closely with ecosystem partners to add dedicated packages for 20+ providers in Python including MongoDB, Mistral, and Together AI – as well as 17 providers in JavaScript including Google VertexAI, Weaviate, and Cloudflare.How to upgradev0.2 contains many improvements, and we designed it to be largely backwards compatible with minimal breaking changes. We’ve also worked to add a migration CLI to ease any issues, as well as documentation highlighting what has changed between versions.Check out our GitHub Discussions thread for details on how to test the CLI and install the v0.2 pre-release. And stay tuned for a full migration guide on the week of May 20th. Why stability matters to usWe value the trust of our 1M+ developers relying on LangChain. As we evolve LangChain, we’re committed to delivering industry-leading solutions while ensuring a foundational framework for engineering teams to confidently use in production. While langchain and langchain-core are currently in a pre-1.0 state, we strive to minimize breaking changes and deprecate classes at least 1 full breaking release ahead of time (3-6 months). Our release cadence also ensures regular updates and bug fixes, keeping the LangChain platform reliable and production-ready. We’ll also continue to maintain a 0.1 version, to which we’ll push critical bug fixes for 3 months. See here for more on our release and deprecation policy.We’d love to hear from you on GitHub on all things LangChain v2.0. And if you’re new to LangChain, follow our quickstart guide to get started. Tags\n",
            "\n",
            "\n",
            "\n",
            "Join our newsletter\n",
            "Updates from the LangChain team and community\n",
            "\n",
            "\n",
            "Enter your email\n",
            "\n",
            "Subscribe\n",
            "\n",
            "Processing your application... Success!' metadata={'_id': '71480e75c97644fc9007b55218e2aeba', '_collection_name': '3f393dc3884e41518f548a249e5f53e9'}\n",
            "----\n",
            "Context:\n",
            "page_content='\n",
            "\n",
            "\n",
            "Towards LangChain 0.1: LangChain-Core and LangChain-Community\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Towards LangChain 0.1: LangChain-Core and LangChain-Community\n",
            "\n",
            "14 min read\n",
            "Dec 12, 2023\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The goal of LangChain has always been to make it as easy as possible to develop context-aware reasoning applications with LLMs. LangChain started as a side project, and purely as a Python package.' metadata={'_id': '5d1a723abf504c8a8b9e860af6a9d8bb', '_collection_name': '3f393dc3884e41518f548a249e5f53e9'}\n",
            "----\n",
            "Context:\n",
            "page_content='\n",
            "\n",
            "\n",
            "Documentation Refresh for LangChain v0.2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Documentation Refresh for LangChain v0.2\n",
            "Learn about the docs refresh for LangChain v0.2. There's now versioned docs and a clearer structure — with tutorials, how-to guides, conceptual guides, and API docs\n",
            "\n",
            "4 min read\n",
            "May 20, 2024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain v0.2 is available to all users today (learn more on the motivation and details here). A major highlight of this launch is our documentation refresh. We wanted to spend some time talking about what the documentation refresh involves and thank community members for the push.Community-driven docs feedbackYou spoke, and we listened. Having heard consistent feedback from the LangChain community and devs that our documentation needed work, we want to address some of the key issues raised and how we’ve tried to improve. We also want to thank several community members in particular for feedback and help.The issues we’ve heard and tried to address include:Constantly changing docs. Keeping up with changes can be painful — so we’ve introduced versioned docs. There’s now documentation tailored to every minor version.Difficult to find relevant information. Instead of a maze of pages, our new docs structure is incredibly flat. It’s organized into four sections (tutorials, how-to-guides, conceptual guides, and API reference) to make the search for info easier and more consistent.Outdated and duplicate content. We’ve cleaned house. Our docs structure now consolidates relevant information and cuts out the clutter of duplicate content. This smaller set of documentation will make it easier to avoid outdated information.Lack of clear instructions on how to update. To provide an evergreen set of instructions, we’ve created a “LangChain over time” doc to outline what has changed with the latest version of LangChain and how to migrate to it.We’ll walk through some of these new features in the blog, but if you want a more hands-on walkthrough, check out the YouTube video we made walking through the new docs! Versioned docsLangChain has evolved considerably from the initial release of the Python package in October of 2022. The documentation has evolved alongside it. These docs updates reflect the new and evolving mental models of how best to use LangChain but can also be disorienting to users.Starting with the v0.2 release, we’re introducing versioned docs for all minor versions. This means that the v0.1 documentation will remain accessible and discoverable for those who prefer it. The list of available documentation versions can be found in the top navigation bar.We hope this strikes a balance between updating our documentation, while not throwing off everyone’s mental model.Documentation structureIn this iteration, we’ve embraced the Diataxis taxonomy to make our docs more clear and user-friendly. Previously, we only partially implemented this, which wasn’t enough. We’ve now adopted this approach fully and have our main documentation fully separated out accordingly.Tutorials are step-by-step guides for how to build specific applications from start to finish with LangChain, like a chatbot, RAG application, or agent. See the image below for an example of a tutorial.How-to guides are detailed instruction guides for how to do particular tasks. This goes more in-depth than a tutorial, covering slightly more advanced topics.Our brand-new conceptual guide is a handy glossary of terminology and a list of different techniques. This is ideal for referencing new concepts or getting the bigger picture of how LangChain works.Finally, our API docs contain detailed technical reference documentation.Together, these changes make the docs structure much more flat (easier to find things) and more consolidated (less likely to have duplicated content).LangChain over timeOur new “LangChain over time” section in the docs helps you stay on top of changes. There are guides on how LangChain has changed, how to upgrade, and how to map previous concepts from old versions to new ones. This has been a much-requested community feature - and we agree that it’s much needed!A huge thank youImproving the docs has been a team and community effort at LangChain. A big shoutout to Leo Gan, our top contributor to the docs. And thank you to everyone in the community for your feedback and help. We will continue to iterate and develop our documentation to help all devs better navigate this new world of LLM applications.LangChain v0.2 is all about improving stability and discoverability. Check out the new docs (Python and JS) – and drop us a line in GitHub. You can also read more about LangChain v0.2 here. Join our newsletter\n",
            "Updates from the LangChain team and community\n",
            "\n",
            "\n",
            "Enter your email\n",
            "\n",
            "Subscribe\n",
            "\n",
            "Processing your application... Success!' metadata={'_id': 'bb51ee704cc24d9db80714041bd0c774', '_collection_name': '3f393dc3884e41518f548a249e5f53e9'}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for context in response[\"context\"]:\n",
        "  print(\"Context:\")\n",
        "  print(context)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagiJ6l6noPL"
      },
      "source": [
        "Let's see if it can handle a query that is totally unrelated to the source documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "HOd2nJKZnsty"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "TmLCKNGZLTh6",
        "outputId": "1478d061-7129-4b14-c717-c99d130a9488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith - {unique_id}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
        "\n",
        "Join [here](https://www.langchain.com/langsmith)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "5560a787-8114-4c72-b95a-6a776043427a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: ··········\n"
          ]
        }
      ],
      "source": [
        "#os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eoqBtBQERXP",
        "outputId": "fc1ea857-6784-4e7e-9fd4-94fbdd9f693a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LangSmith is a framework built on top of LangChain, designed to enhance the development and performance of AI-powered applications, particularly those involving Large Language Models (LLMs). It provides tools for tracking, fine-grain control, and customizability, which are crucial for improving the reliability and observability of LLM-based systems. LangSmith includes features such as an SDK for easy integration and a tracing mechanism to log runs and iterations, facilitating rapid development and optimization. It is utilized by various teams, such as Athena Intelligence and Wordsmith, to accelerate prompt engineering, improve product quality, and manage complex optimization challenges in their AI workflows.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 127, 'prompt_tokens': 901, 'total_tokens': 1028}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'stop', 'logprobs': None}, id='run-d1014568-6951-4672-9f8a-399ff22691ff-0', usage_metadata={'input_tokens': 901, 'output_tokens': 127, 'total_tokens': 1028})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52o58AfsLK6"
      },
      "source": [
        "#### 🏗️ Activity #1:\n",
        "\n",
        "Include a screenshot of your trace and explain what it means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Task 5: Loading Our Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsJ8uCbT7S1Z",
        "outputId": "cdcbe030-aae2-46da-8cb7-30811da9f87d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python(37529) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DataRepository'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 84 (delta 23), reused 28 (delta 8), pack-reused 8 (from 1)\u001b[K\n",
            "Receiving objects: 100% (84/84), 70.08 MiB | 18.89 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AI-Maker-Space/DataRepository.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w5fmy5Gy7X03"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"DataRepository/langchain_blog_test_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>idx</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>How did Podium improve their agent F1 response...</td>\n",
              "      <td>How Podium optimized agent behavior and reduce...</td>\n",
              "      <td>0</td>\n",
              "      <td>Podium optimized agent behavior and reduced en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>How did Athena Intelligence utilize LangSmith ...</td>\n",
              "      <td>How Athena Intelligence optimized research rep...</td>\n",
              "      <td>40</td>\n",
              "      <td>Athena Intelligence used the LangSmith playgro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>What are the four strategies supported by Lang...</td>\n",
              "      <td>and with LangGraph and LangSmith, LangChain de...</td>\n",
              "      <td>80</td>\n",
              "      <td>LangGraph Cloud provides four different strate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>What action is required after receiving the su...</td>\n",
              "      <td>Tags\\n\\n\\n\\nJoin our newsletter\\nUpdates from ...</td>\n",
              "      <td>120</td>\n",
              "      <td>Please check your inbox and click the link to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>What are the key features of the Open Source E...</td>\n",
              "      <td>Open Source Extraction Service\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>160</td>\n",
              "      <td>The Open Source Extraction Service is a newly ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>What are the main subsections of Gödel's mathe...</td>\n",
              "      <td>{'article_h1_main': 'Kurt Gödel', 'article_h2_...</td>\n",
              "      <td>200</td>\n",
              "      <td>The question isn't provided, but based on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>What should you do after receiving the \"Succes...</td>\n",
              "      <td>Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nU...</td>\n",
              "      <td>240</td>\n",
              "      <td>Subscribe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>What is the role of the ISUSE token in the Sel...</td>\n",
              "      <td>output is yes, no, continueISREL token decides...</td>\n",
              "      <td>280</td>\n",
              "      <td>The ISUSE token decides whether the generation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>What should enterprises do if they are looking...</td>\n",
              "      <td>strategies to get the Assistant architecture t...</td>\n",
              "      <td>320</td>\n",
              "      <td>gtm@langchain.dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>What is the purpose of using LangChain in the ...</td>\n",
              "      <td>As 2023 comes to a close, Graphite wanted to c...</td>\n",
              "      <td>360</td>\n",
              "      <td>Year in code is a personalized, AI-generated v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>Explain the difference between \"pulling\" and \"...</td>\n",
              "      <td>that the API doesn’t call the tools for you - ...</td>\n",
              "      <td>400</td>\n",
              "      <td>First, many are actually NOT this “agent” cogn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>What file format needs to be migrated accordin...</td>\n",
              "      <td>\"id\": \"599d75c8-517c-4f37-88df-ff16576bd607\",\\...</td>\n",
              "      <td>440</td>\n",
              "      <td>The acceptance-test-config.yml file is in a le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>What are the two required inputs for an OpenAI...</td>\n",
              "      <td>How to design an Agent for Production\\n\\n\\n\\n\\...</td>\n",
              "      <td>480</td>\n",
              "      <td>The purpose of the guide \"How to design an Age...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>What is the name of the AI-powered developer a...</td>\n",
              "      <td>Robocorp’s code generation assistant makes bui...</td>\n",
              "      <td>520</td>\n",
              "      <td>ReMark helps practitioners by generating funct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>What is one of the main advantages of using Op...</td>\n",
              "      <td>response = chain.invoke(text)In the LangChain ...</td>\n",
              "      <td>560</td>\n",
              "      <td>Option #1: do not save inputs or outputs\\n\\nIf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>What specific business requirements does Eden ...</td>\n",
              "      <td>Editor's Note: This post was written in collab...</td>\n",
              "      <td>600</td>\n",
              "      <td>Question: What are the main advantages of inte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>What role does the RecordManager play in the L...</td>\n",
              "      <td>Syncing data sources to vector stores\\n\\n\\n\\n\\...</td>\n",
              "      <td>640</td>\n",
              "      <td>The LangChain Indexing API helps avoid writing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>What are the two concrete steps mentioned in t...</td>\n",
              "      <td>LangChainHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>680</td>\n",
              "      <td>The LangChainHub is a platform designed to mak...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>Explain how the previous implementation of run...</td>\n",
              "      <td>Callbacks Improvements\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>720</td>\n",
              "      <td>The improvements to the callbacks system in La...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>What are the two main benefits provided by the...</td>\n",
              "      <td>GPT Researcher x LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>760</td>\n",
              "      <td>GPT Researcher generates research reports by u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>What action should you take after receiving a ...</td>\n",
              "      <td>Tags\\n\\n\\n\\nJoin our newsletter\\nUpdates from ...</td>\n",
              "      <td>800</td>\n",
              "      <td>\"Success! Please check your inbox and click th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>What recent measures have been taken to addres...</td>\n",
              "      <td>a specific task)Future PlansAs mentioned earli...</td>\n",
              "      <td>840</td>\n",
              "      <td>LangChain will continue to add features and pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>What are the two key facets of the MVP browser...</td>\n",
              "      <td>[Editor's Note]: This is the second of hopeful...</td>\n",
              "      <td>880</td>\n",
              "      <td>The authors of \"Origin\" are currently pursuing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0                                           question  \\\n",
              "0            0  How did Podium improve their agent F1 response...   \n",
              "1            1  How did Athena Intelligence utilize LangSmith ...   \n",
              "2            2  What are the four strategies supported by Lang...   \n",
              "3            3  What action is required after receiving the su...   \n",
              "4            4  What are the key features of the Open Source E...   \n",
              "5            5  What are the main subsections of Gödel's mathe...   \n",
              "6            6  What should you do after receiving the \"Succes...   \n",
              "7            7  What is the role of the ISUSE token in the Sel...   \n",
              "8            8  What should enterprises do if they are looking...   \n",
              "9            9  What is the purpose of using LangChain in the ...   \n",
              "10          10  Explain the difference between \"pulling\" and \"...   \n",
              "11          11  What file format needs to be migrated accordin...   \n",
              "12          12  What are the two required inputs for an OpenAI...   \n",
              "13          13  What is the name of the AI-powered developer a...   \n",
              "14          14  What is one of the main advantages of using Op...   \n",
              "15          15  What specific business requirements does Eden ...   \n",
              "16          16  What role does the RecordManager play in the L...   \n",
              "17          17  What are the two concrete steps mentioned in t...   \n",
              "18          18  Explain how the previous implementation of run...   \n",
              "19          19  What are the two main benefits provided by the...   \n",
              "20          20  What action should you take after receiving a ...   \n",
              "21          21  What recent measures have been taken to addres...   \n",
              "22          22  What are the two key facets of the MVP browser...   \n",
              "\n",
              "                                              context  idx  \\\n",
              "0   How Podium optimized agent behavior and reduce...    0   \n",
              "1   How Athena Intelligence optimized research rep...   40   \n",
              "2   and with LangGraph and LangSmith, LangChain de...   80   \n",
              "3   Tags\\n\\n\\n\\nJoin our newsletter\\nUpdates from ...  120   \n",
              "4   Open Source Extraction Service\\n\\n\\n\\n\\n\\n\\n\\n...  160   \n",
              "5   {'article_h1_main': 'Kurt Gödel', 'article_h2_...  200   \n",
              "6   Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nU...  240   \n",
              "7   output is yes, no, continueISREL token decides...  280   \n",
              "8   strategies to get the Assistant architecture t...  320   \n",
              "9   As 2023 comes to a close, Graphite wanted to c...  360   \n",
              "10  that the API doesn’t call the tools for you - ...  400   \n",
              "11  \"id\": \"599d75c8-517c-4f37-88df-ff16576bd607\",\\...  440   \n",
              "12  How to design an Agent for Production\\n\\n\\n\\n\\...  480   \n",
              "13  Robocorp’s code generation assistant makes bui...  520   \n",
              "14  response = chain.invoke(text)In the LangChain ...  560   \n",
              "15  Editor's Note: This post was written in collab...  600   \n",
              "16  Syncing data sources to vector stores\\n\\n\\n\\n\\...  640   \n",
              "17  LangChainHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  680   \n",
              "18  Callbacks Improvements\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  720   \n",
              "19  GPT Researcher x LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  760   \n",
              "20  Tags\\n\\n\\n\\nJoin our newsletter\\nUpdates from ...  800   \n",
              "21  a specific task)Future PlansAs mentioned earli...  840   \n",
              "22  [Editor's Note]: This is the second of hopeful...  880   \n",
              "\n",
              "                                               answer  \n",
              "0   Podium optimized agent behavior and reduced en...  \n",
              "1   Athena Intelligence used the LangSmith playgro...  \n",
              "2   LangGraph Cloud provides four different strate...  \n",
              "3   Please check your inbox and click the link to ...  \n",
              "4   The Open Source Extraction Service is a newly ...  \n",
              "5   The question isn't provided, but based on the ...  \n",
              "6                                           Subscribe  \n",
              "7   The ISUSE token decides whether the generation...  \n",
              "8                                   gtm@langchain.dev  \n",
              "9   Year in code is a personalized, AI-generated v...  \n",
              "10  First, many are actually NOT this “agent” cogn...  \n",
              "11  The acceptance-test-config.yml file is in a le...  \n",
              "12  The purpose of the guide \"How to design an Age...  \n",
              "13  ReMark helps practitioners by generating funct...  \n",
              "14  Option #1: do not save inputs or outputs\\n\\nIf...  \n",
              "15  Question: What are the main advantages of inte...  \n",
              "16  The LangChain Indexing API helps avoid writing...  \n",
              "17  The LangChainHub is a platform designed to mak...  \n",
              "18  The improvements to the callbacks system in La...  \n",
              "19  GPT Researcher generates research reports by u...  \n",
              "20  \"Success! Please check your inbox and click th...  \n",
              "21  LangChain will continue to add features and pr...  \n",
              "22  The authors of \"Origin\" are currently pursuing...  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBtPs7p9Mbz"
      },
      "source": [
        "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
        "\n",
        "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-aie4-triples-v4\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for triplet in test_df.iterrows():\n",
        "  triplet = triplet[1]\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : triplet[\"question\"], \"context\": triplet[\"context\"]},\n",
        "      outputs={\"answer\" : triplet[\"answer\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "## Task 6: Evaluation\n",
        "\n",
        "Now we can run the evaluation!\n",
        "\n",
        "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
        "\n",
        "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "fbjnv3bMwQKg"
      },
      "outputs": [],
      "source": [
        "def prepare_data_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.outputs[\"answer\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_data_noref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_context_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.inputs[\"context\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwnMdtl9nwz"
      },
      "source": [
        "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
        "\n",
        "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "501c5e204387460e8e9a585c9b2ca834",
            "323a077888c84bdfbac393d1ebef9d6c",
            "710fe93f3241401bafa10a1e0a1bda02",
            "c65a6921701742da9b67c591b19b6336",
            "17d4193e78a14050a11c5f8306b3ba0b",
            "99751d16888640078ae4820cacab5760",
            "a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "54e7ccad2e774261a3313316a1397f66",
            "41e287b8dc674f839b6485c9606207d5",
            "f7457f38948d472da4027ba1566b47e4",
            "af61b863014d4a8c960570de31a02b3f"
          ]
        },
        "id": "CENtd4K_IQa3",
        "outputId": "872581e4-9c21-414f-d25e-9454c47c0587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Base RAG Evaluation-bd0ce260' at:\n",
            "https://smith.langchain.com/o/a6e0035c-78ce-5cad-a3a2-da78121a4e95/datasets/2fc949bb-9134-4fb3-8c40-bc6964de7bad/compare?selectedSessions=ee1d0d63-c592-48d0-ab55-162a23650d1d\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run 85063234-a4f0-4d49-b59a-9c440a859d58: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39662, Requested 339. Please try again in 1ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39662, Requested 339. Please try again in 1ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "2it [00:25, 10.39s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 769043cf-d832-4f9f-b105-49b936875864: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39573, Requested 529. Please try again in 153ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39573, Requested 529. Please try again in 153ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "7it [00:27,  1.50s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run fc54f2cb-a073-4796-85cf-96ab3c314d52: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39863, Requested 732. Please try again in 892ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39863, Requested 732. Please try again in 892ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "8it [00:28,  1.16s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 8e79a0b8-462a-4807-857e-a2254fc3510e: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9925, Requested 570. Please try again in 2.97s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/scoring/eval_chain.py\", line 351, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9925, Requested 570. Please try again in 2.97s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "9it [00:28,  1.13it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run 80c70ea9-22ce-4744-8216-1182d8a3d346: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39413, Requested 1497. Please try again in 1.365s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39413, Requested 1497. Please try again in 1.365s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "10it [00:29,  1.01it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run 335d8709-dca9-48dc-919f-6c81d9c6cd32: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39497, Requested 1910. Please try again in 2.11s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39497, Requested 1910. Please try again in 2.11s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 32f63c2b-fae5-4406-bc52-237291497de7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39878, Requested 1218. Please try again in 1.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39878, Requested 1218. Please try again in 1.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2d75f1e1-5e02-40a2-836c-1e7fea83160d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39824, Requested 1286. Please try again in 1.665s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 39824, Requested 1286. Please try again in 1.665s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "12it [00:31,  1.14it/s]Error running evaluator <DynamicRunEvaluator evaluate> on run fe2236c7-5480-4f29-8e23-95fceb5d73c8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 38748, Requested 1777. Please try again in 787ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/qa/eval_chain.py\", line 303, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 40000, Used 38748, Requested 1777. Please try again in 787ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "13it [00:34,  1.42s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 80c70ea9-22ce-4744-8216-1182d8a3d346: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9497, Requested 504. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9497, Requested 504. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2d75f1e1-5e02-40a2-836c-1e7fea83160d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9523, Requested 601. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9523, Requested 601. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 335d8709-dca9-48dc-919f-6c81d9c6cd32: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9660, Requested 637. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9660, Requested 637. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "14it [00:36,  1.57s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run 32f63c2b-fae5-4406-bc52-237291497de7: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9425, Requested 749. Please try again in 1.044s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9425, Requested 749. Please try again in 1.044s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "17it [00:41,  1.71s/it]Error running evaluator <DynamicRunEvaluator evaluate> on run fe2236c7-5480-4f29-8e23-95fceb5d73c8: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9521, Requested 932. Please try again in 2.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1266, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 278, in evaluate_run\n",
            "    result = self.func(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 603, in wrapper\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 600, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 260, in evaluate\n",
            "    results = self.evaluator.evaluate_strings(**eval_inputs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/schema.py\", line 220, in evaluate_strings\n",
            "    return self._evaluate_strings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 445, in _evaluate_strings\n",
            "    result = self(\n",
            "             ^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 170, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 381, in __call__\n",
            "    return self.invoke(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 164, in invoke\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/base.py\", line 154, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n",
            "    raise e\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 623, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 845, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 635, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 668, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1260, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 937, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1026, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1075, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/anaconda3/envs/aie4/lib/python3.12/site-packages/openai/_base_client.py\", line 1041, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-BftgYQeTR9aYOsNnaKMgI8Vd on tokens per min (TPM): Limit 10000, Used 9521, Requested 932. Please try again in 2.718s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "23it [01:02,  2.72s/it]\n"
          ]
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", prepare_data=prepare_context_ref)\n",
        "\n",
        "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\" : {\n",
        "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
        "        }\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "labeled_score_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
        "        },\n",
        "    },\n",
        "    prepare_data=prepare_data_ref\n",
        ")\n",
        "\n",
        "base_rag_results = evaluate(\n",
        "    retrieval_augmented_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        ],\n",
        "    experiment_prefix=\"Base RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPocfrNFiYWi"
      },
      "source": [
        "#### ❓Question #1:\n",
        "\n",
        "What conclusions can you draw about the above results?\n",
        "\n",
        "`cot_qa_evaluator`: </br>\n",
        "Uses the `prepare_context_ref` function </br>\n",
        "This evaluator is assessing the chain-of-thought (CoT) quality of the QA model </br>\n",
        "\n",
        "`unlabeled_dopeness_evaluator`: </br>\n",
        "Uses the `prepare_data_noref` function </br>\n",
        "This is a custom evaluator that assesses the \"dopeness\" of the answer </br>\n",
        "It doesn't use a reference answer, instead judging the coolness or awesomeness of the response based on the question and prediction alone </br>\n",
        "\n",
        "`labeled_score_evaluator`: </br>\n",
        "Uses the `prepare_data_ref` function </br>\n",
        "This evaluator is assessing the accuracy of the QA model by comparing the generated answer with the reference answer </br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17d4193e78a14050a11c5f8306b3ba0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "323a077888c84bdfbac393d1ebef9d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99751d16888640078ae4820cacab5760",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "value": ""
          }
        },
        "41e287b8dc674f839b6485c9606207d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "501c5e204387460e8e9a585c9b2ca834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_323a077888c84bdfbac393d1ebef9d6c",
              "IPY_MODEL_710fe93f3241401bafa10a1e0a1bda02",
              "IPY_MODEL_c65a6921701742da9b67c591b19b6336"
            ],
            "layout": "IPY_MODEL_17d4193e78a14050a11c5f8306b3ba0b"
          }
        },
        "54e7ccad2e774261a3313316a1397f66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "710fe93f3241401bafa10a1e0a1bda02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e7ccad2e774261a3313316a1397f66",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41e287b8dc674f839b6485c9606207d5",
            "value": 1
          }
        },
        "99751d16888640078ae4820cacab5760": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f5f4beaa8f4a3b94002c5ed122c2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af61b863014d4a8c960570de31a02b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c65a6921701742da9b67c591b19b6336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7457f38948d472da4027ba1566b47e4",
            "placeholder": "​",
            "style": "IPY_MODEL_af61b863014d4a8c960570de31a02b3f",
            "value": " 23/? [02:10&lt;00:00,  5.36s/it]"
          }
        },
        "f7457f38948d472da4027ba1566b47e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
